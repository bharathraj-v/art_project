# Significance of Art

Since prehistoric times, human beings have closely linked any and all forms of their work with some sort of artistic interpretation. Art, in all of its forms, plays a major role in how humans see and interact with each other and their environment. The earliest known examples of art are [dated to 30000 BP](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2815939/), during the later stone age. This implies how art, in all its forms, allows us to express ourselves and take in perspectives that cannot be expressed via words.

Ever since its inception, art has crossed any and all limitations, from physical to intangible forms, i.e., people of all cultures, races, and ethnicities have some form of message or interpretation of any kind of artwork in their respective culture. Art has not only influenced aesthetes and artists to make more of it but has also helped record major historic events and narratives of the past, that would otherwise be long lost to trivial human wars and natural calamities.

<img src="https://gohighbrow.com/wp-content/uploads/2015/05/10-No.-5.jpg" alt="Jackson Pollock's No.5, 1948" style="zoom:50%;" />

​									 															             [_No.5 by Jackson Pollock_](https://www.jackson-pollock.org/number-5.jsp)

Over the years, the enduring appeal of art has changed from just admiration to articulation, intellectual contemplation, and depiction of the state of human society. However, the thing that abstract art stands for, which is triggering an emotional response from its viewers and letting them ponder about its meaning, is still very much intact.

Abstract art doesn't necessarily depict anything that the real world has. It can simply be an expression of language or emotion using colors, patterns forms and styles. 

The best part about art is that all kinds of people experience it differently. One might simply admire the color schemes of a painting, while the other may look at the strokes and patterns it leads to. It starts out as a certain thought in the hands of the artist, and ends up as a completely different, unique thought in the eyes of the beholder.





# Recent AI Breakthroughs

Although the role of AI in art creation and the relationship between artists and AI is still evolving, some very significant leaps have been made during the past decade, and remarkable feats like [computers beating humans at image recognition](https://image-net.org/challenges/LSVRC/2015/ilsvrc2015_perclass_results.tar.gz), at complex games with a mathematically large amount of patterns and outcomes like [chess and go](https://arxiv.org/pdf/1712.01815.pdf), [AI models generating text](https://arxiv.org/pdf/2005.14165.pdf) and [artistic images](https://arxiv.org/ftp/arxiv/papers/2205/2205.02439.pdf) have already been achieved. It's extremely intriguing how well AI can mimic and surpass human capabilities in the aforementioned activities.



## Deep Learning

### Neural Networks

[Neural networks](https://arxiv.org/pdf/1901.05639.pdf), also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of machine learning and are at the heart of deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that [biological neurons](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1559659/) signal to one another.

Artificial neural networks (ANNs) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.

Neural networks rely on training data to learn and improve their accuracy over time, and once these learning algorithms are fine-tuned for accuracy, they can be used to classify and cluster data at a high velocity. Tasks in speech recognition or image recognition, which would take hours by human experts, would take mere minutes by such neural networks.

A neural network that consists of more than three layers, which would be inclusive of the inputs and the output, can be considered a deep learning algorithm.



### Deep Learning

[Deep learning](https://arxiv.org/pdf/2106.10165.pdf) is a subset of machine learning inspired by the brain's network of neurons. And while neural networks have been around for decades, one of the most well known advancements for deep learning happened in 2012, called the [ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](https://arxiv.org/pdf/1409.0575.pdf). It was a pivotal moment for the use of deep neural networks for image recognition. 

Another famous model is [Google's BERT](https://arxiv.org/pdf/1810.04805.pdf), a language model based on [transformers](https://arxiv.org/pdf/1706.03762.pdf) to help the search engine get better context around its users' searches. It was released in 2018 with 12 layers, 12 attention heads and 110 million parameters. [Language models based on RNN](https://www.researchgate.net/publication/221489926_Recurrent_neural_network_based_language_model#read) faced issues like parallelizing and retaining contextual connections. Since context is key in [NLP](https://www.researchgate.net/publication/319164243_Natural_Language_Processing_State_of_The_Art_Current_Trends_and_Challenges#read), BERT easily bypassed these problems, and the results were far better than the state-of-the-art RNN models. But this was just the beginning of transformer based neural networks, as Open AI's GPT-3 was shockingly good and even better than BERT.



## GPT-3: AI gets language

On 11 June 2020, [OpenAI](https://openai.com/about/) released the biggest language model ever known, [GPT-3](https://openai.com/api/) along with its [API documentation](https://beta.openai.com/docs/). Trained on 175 billion parameters, which is 116 times more than it's predecessor GPT-2, the language prediction model has the capability to serve a wide range of purposes such as creating blog posts, advertisements, even poetry that mimics the style of famous poets like Shakespeare, Edgar Allan Poe, so realistic that it can easily be looked at as something written by an actual human being. It can even generate text summarizations and code. GPT-3 is trained to perform such tasks with [small amounts of input text while maintaining context and producing large amounts of output](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf). 

Around September 2020, The Guardian had used GPT-3 to write [an article about AI being harmless to human beings](https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3). The article really shows its proficiency, and is a spectacle to witness in the world of language generation.



## DALL-E: AI gets artsy

[DALL-E](https://openai.com/blog/dall-e/) is a multimodal implementation of GPT-3 with 12 billion parameters, which uses [zero-short learning](https://arxiv.org/pdf/2102.12092.pdf) to generate images and art based on text prompts of descriptions and desired styles. Trained on text-image pairs from the internet, DALL-E creates multiple responses to any text input, and [CLIP](https://arxiv.org/pdf/2103.00020.pdf), an image recognition system, understands and ranks these images to associate the most appropriate image to the caption (text).

![img](https://cdn.openai.com/dall-e/v2/samples/product_design/YW4gYXJtY2hhaXIgaW4gdGhlIHNoYXBlIG9mIGFuIGF2b2NhZG8uIGFuIGFybWNoYWlyIGltaXRhdGluZyBhbiBhdm9jYWRvLg==_4.png)

​																				[_An armchair in the shape of an avocado, generated by DALL-E_](https://cdn.openai.com/dall-e/v2/samples/product_design/YW4gYXJtY2hhaXIgaW4gdGhlIHNoYXBlIG9mIGFuIGF2b2NhZG8uIGFuIGFybWNoYWlyIGltaXRhdGluZyBhbiBhdm9jYWRvLg==_4.png)

Its successor, [DALL-E 2](https://openai.com/dall-e-2/) shows even better results in terms of photorealism and caption matching, unlocking a vast potential for generation of art using AI systems. 

<img src="https://cdn.openai.com/dall-e-2/demos/text2im/soup/portal/digital_art/0.jpg" alt="unknown" style="zoom: 25%;" />

​												         [_A bowl of soup that is a portal to another dimension as digital art, generated by DALL-E 2_](https://cdn.openai.com/dall-e-2/demos/text2im/soup/portal/digital_art/0.jpg)





# Generative Art

As inferred from [Prof. Philip Galanter's paper](http://www.philipgalanter.com/downloads/ga2003_paper.pdf), the term 'Generative Art' refers to the kind of art where the artist uses a system, like a set of natural language rules, a computer program, a machine, or other procedural invention, set into motion with some degree of autonomy contributing or resulting in a finished work of art. An [autonomous system](http://www.generativeart.com/on/cic/papersga2007/07.pdf) in the context of generative art is a system that is non-human and can independently determine features of an artwork that would otherwise require decisions made directly by the artist. 

One of the first known examples of generative art is a musical game called **Musikalisches Würfelspiel** in which dice were rolled to randomly select already-composed fragments of music, which were strung together to form a finished piece. This game was played in Berlin in 1792, and has been attributed to the famous composer Wolfgang Amadeus Mozart, hence, this being the inspiration behind the model being named 'Mozart'.

Another example of generative art is the set of Italian Medieval town designs created by an architect named Celestino Soddu in 1987. He created a set of conditions where a random computer process could be set in motion to create a model of a town. The conditions were such that the final result would always be a town identifiable in the Italian Medieval style. Despite there being enough constraints on the models to keep them in this style, an essentially infinite number of models could be created.

As shown in the above example, the beauty of generative art is that the possibilities for creation are limitless. A process with certain conditions created by the user are set into motion and the element of chance creates the most intruiging piece of digital art. 

During the 1960s, when generative art became quite popular, famous digital artist [Harold Cohen](https://computerhistory.org/blog/harold-cohen-and-aaron-a-40-year-collaboration/) became interested in work by computer scientists at the University of San Diego. Programmers created a system on punched cards, fed those cards into a machine which would then return results via a set of newly punched cards, or prints. He applied this technology to computer-controlled drawing machines, which he called "turtles". Cohen programmed the “turtles” to follow a set of processes which would lead to a piece of art being formed on canvas. His system, [AARON](http://www.haroldcohen.com/aaron/publications/how2make.pdf), is one of the longest-running, continually maintained AI systems in history.

<img src="http://images.computerhistory.org/blog-media/harold_cohen_aaron_IMG_6638.jpg" alt="Detail from an untitled AARON drawing, ca. 1980." style="zoom:50%;" />

​																						 [_Detail from an untitled AARON drawing, ca. 1980._](https://images.computerhistory.org/blog-media/harold_cohen_aaron_IMG_6638.jpg)

However, Cohen has further explained in his paper - [The further exploits of AARON, Painter](https://web.archive.org/web/20060107184824/http://crca.ucsd.edu/~hcohen/cohenpdf/furtherexploits.pdf) regarding the artistic legitimacy and functions of what his system is exactly doing, and whether it's justified to call it 'creative'. 

Another prominent artist named Vera Molnar, during the same time as Cohen, had been experimenting with creating images using "machine imaginaire", as she called it. She created a set of rules to paint a series of geometric images, allowing her to explore endless shapes and lines. 

<img src="https://images.squarespace-cdn.com/content/v1/59413d96e6f2e1c6837c7ecd/1534897464622-0LL0T4WAII94R4FIV49B/Interruptions+-+Vera+Moln%C3%A1r%2C+1968%2F69?format=750w" alt="img" style="zoom:50%;" />

​                                                                                                           [_Interruptions - Vera Molnar_](https://images.squarespace-cdn.com/content/v1/59413d96e6f2e1c6837c7ecd/1534897464622-0LL0T4WAII94R4FIV49B/Interruptions+-+Vera+Molnár%2C+1968%2F69?format=750w)





## Deep Learning Models

In order to attain accuracy that surpasses that of humans, deep learning models learn to execute categorization tasks directly from images, text, or sound. Deep learning models are trained using an objective function, but how that function is constructed reveals a lot about the model's intended use.

[Deep neural networks](https://arxiv.org/pdf/1404.7828.pdf), which are used to process data in complicated ways using extensive mathematical modelling, are far more complex than the simple neural networks utilized in the majority of machine learning techniques. Deep neural networks take advantage of the hidden layer component that is included in ANNs by stacking an increasing number of those layers in order to assess the importance of the input to the output and create new associations between various input combinations.

<img src="https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png" alt="img" style="zoom: 33%;" />

​																		[*Pictorial demonstration of a neural network in a Deep Learning Model*](https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png)

Image identification, speech recognition, natural language processing, and many more applications all heavily rely on deep learning models. Deep learning is also the foundation for the great majority of [Machine Reading Comprehension (MRC) models](https://arxiv.org/ftp/arxiv/papers/2001/2001.01582.pdf) now being used.

Machine learning's branch of deep learning models differs from other branches for a variety of reasons:

- The majority of deep learning models are very sophisticated. Artificial neural networks (ANN), on which deep learning is based, have the property that their model size can be controlled. This means that even with a fixed input dimension, the number of model parameters can be controlled by varying the number of network layers, the number of connections, and the layer size. Deep learning makes it simple to expand model complexity as a result, enabling more effective utilization of enormous amounts of data. A higher amount of data can also boost the accuracy of deep learning models. Deep learning has been the most popular machine learning architecture for reading comprehension as the area of MRC continues to develop and new datasets are created.

- [Feature learning](https://arxiv.org/pdf/1301.3605.pdf) is a strong quality of deep learning. In machine learning, a model's performance is heavily influenced by [representation learning](https://arxiv.org/pdf/1206.5538.pdf)—the process by which a model learns a good representation of the data. The extraction of task-specific characteristics is an established process for traditional machine learning models. Prior to the development of deep learning, [feature extraction](https://www.researchgate.net/publication/287743399_A_survey_of_feature_selection_and_feature_extraction_techniques_in_machine_learning#read) was frequently manual and called for subject-matter expertise. Deep learning, on the other hand, uses neural networks to automatically learn useful feature representations by performing a [nonlinear transformation](https://www.researchgate.net/publication/307888996_On_the_Role_of_Nonlinear_Transformations_in_Deep_Neural_Network_Acoustic_Models#read) on the basic input features, such as word vectors and image pixels. To put it another way, deep learning successfully acquires salient features that are helpful to the target task without requiring the model designers to have specialized domain expertise, which considerably improves the efficiency of creating deep learning models for tasks from many applications.

- [End-to-end learning](https://towardsdatascience.com/e2e-the-every-purpose-ml-method-5d4f20dafee4) is achieved by deep learning. In the past, many machine learning models suggested [pipelines](https://www.researchgate.net/publication/332414216_Continuous_Deployment_of_Machine_Learning_Pipelines#read) for multistep solutions. To simultaneously optimise the entire system to increase performance, however, is challenging because each phase can only be addressed independently. The efficiency is also considerably decreased if any step in the model is altered because it is likely that all subsequent phases will need to be modified as well. The [featurization](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-features) ability of neural networks end-to-end learning, which feeds in the raw data as input and outputs the needed result, can overcome this problem by optimising all parameters in an organised fashion to increase accuracy. For instance, in MRC, the model inputs the text of the article and the question and outputs the text of the response. It is very simple to use and deploy, substantially simplifying the optimization process.

- Deep learning frequently makes advantage of enhanced hardware, particularly the [Graphics Processing Unit (GPU)](https://www.researchgate.net/publication/325023664_Performance_of_CPUsGPUs_for_Deep_Learning_workloads#read). Because deep learning models are typically big, computational efficiency has become a key driver of deep learning advancement. The computation has been significantly expedited by the GPU's better design. The floating-point computational capability, read/write speed, and [parallelism](https://www.researchgate.net/publication/256495766_A_Survey_on_Parallel_Computing_and_its_Applications_in_Data-Parallel_Problems_Using_GPU_Architectures#read) of the GPU are all higher than those of the CPU. The exponential growth of computational power and device complexity over time is known as [Moore's law](https://www.synopsys.com/glossary/what-is-moores-law.html#:~:text=Definition,doubles%20about%20every%20two%20years.), and the development of GPUs over the past ten years has followed this trend. Deep learning-specific GPUs are still being developed by the GPU industry, which is comprised of companies like NVIDIA, Intel, and Google. This activity contributes to the advancement and use of the deep learning field as a whole.

- The emergence and prosperity of deep learning frameworks and community immensely help prompt the booming of deep learning. With the advent of frameworks, such as [TensorFlow](https://www.tensorflow.org/api_docs), , and , neural networks can be automatically optimized and the most commonly used network modules are predefined, making deep learning development much simpler. Meanwhile, deep learning communities quickly thrive. Every time a new research result appears, there will be developers who immediately implement, validate, and open source models, making the popularization of new technologies to be at an unprecedented level. Academic paper repositories like  and open-source code platforms like greatly facilitate the communication between researchers and developers, and considerably lower the threshold for participation in deep learning research.

- Deep learning has greatly benefited from the creation and growth of deep learning frameworks and communities. With the introduction of frameworks like [TensorFlow](https://www.tensorflow.org/api_docs), [PyTorch](https://pytorch.org/docs/stable/index.html) and [Keras](https://keras.io/api/), neural networks can now be automatically tuned, and the most popular network modules are predefined, greatly simplifying the building of deep learning systems. Deep learning groups, meanwhile, grow swiftly. Developers build, validate, and contribute to open source models as soon as a new study finding is published, causing the adoption of new technology to reach previously unheard-of levels. A repository for academic papers called [arXiv](https://arxiv.org/help/api/user-manual), and [GitHub](https://docs.github.com/en), a platform for open-source code, both significantly lower the entry barrier for participation in deep learning research and vastly facilitate communication between academicians and developers.

  

### Convolutional Neural Networks 

The breakthroughs in the field of [computer vision ](https://www.ibm.com/in-en/topics/computer-vision)with deep learning have been built and polished with focus on one particular algorithm - a Convolutional Neural Network.

A [Convolutional Neural Network (ConvNet/CNN)](https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-cnns-illustrated-explanation/#) is a Deep Learning system that can take in an input image, give importance (learnable weights and biases) to various characteristics and objects in the image, and distinguish between them. In comparison to other classification methods, a ConvNet requires substantially less pre-processing. ConvNets can learn these filters and attributes with enough training, unlike basic approaches where filters must be hand-engineered.

In their individual contributions from [1980](https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf) and [1989](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf), [Kunihiko Fukushima](https://dblp.org/pid/19/6731.html) and [Yann LeCun](https://engineering.nyu.edu/faculty/yann-lecun) laid the groundwork for current research on convolutional neural networks.

The structure of a ConvNet was influenced by the way the [Visual Cortex is organized](https://arxiv.org/ftp/arxiv/papers/2001/2001.07092.pdf)  and is similar to the connectivity pattern of neurons in the human brain. Only in this constrained area of the visual field, known as the [Receptive Field](https://arxiv.org/pdf/1701.04128.pdf), do individual neurons react to stimuli. The entire visual area is covered by a series of such fields that overlap.

<img src="https://miro.medium.com/max/1400/1*uAeANQIOQPqWZnnuH-VEyw.jpeg" alt="A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way | by  Sumit Saha | Towards Data Science" style="zoom: 33%;" />

​																			   			 [_A CNN sequence to classify handwritten digits_](https://miro.medium.com/max/1400/1*uAeANQIOQPqWZnnuH-VEyw.jpeg)

Backpropagation was successfully used by Yann LeCun to train neural networks to find and recognise patterns in a collection of handwritten zip codes. He and his colleagues achieved their research goals in the 1990s by using [“LeNet-5”](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf), which utilised the same concepts as earlier work for document recognition.

Utilizing concepts from linear algebra, notably matrix multiplication, convolutional neural networks offer a scalable method for completing picture classification and object recognition tasks. However, they can be computationally taxing, necessitating the use of graphics processing units (GPUs) when modelling them. The ConvNet's job is to condense the images into a format that is simpler to analyze without sacrificing elements that are essential for obtaining an accurate forecast.

They have three main types of layers, which are:

- **Convolutional layer** - A convolutional network's first layer is the convolutional layer. It is the central component of a CNN and the location of most computation. It needs input data, a filter, and a feature map, among other things. It is implied that the input will contain three dimensions—a height, a width, and a depth—which equate to RGB in an image if the input is a colour image, which is composed of a matrix of pixels in three dimensions. Additionally, there is a feature detector, also referred to as a [kernel](https://arxiv.org/pdf/1406.3332.pdf) or filter, which will move through the image's receptive fields and determine whether the feature is there. This is what is called convolution. 

  A two-dimensional (2-D) array of weights serving as the feature detector represents a portion of the image. The filter size, which also controls the size of the receptive field, is normally a 3x3 matrix, however it can vary in size. Following the application of the filter to a portion of the image, the dot product between the input pixels and the filter is determined. The output array is then fed with this dot product. Once the kernel has swept through the entire image, the filter shifts by a stride and repeats the operation. A feature map, activation map, or convolved feature is the ultimate result of the series of dot products from the input and the filter.

  

  <img src="https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_02_17A-ConvolutionalNeuralNetworks-WHITEBG.png" alt="matrix multiplication in convolutional neural networks" style="zoom: 33%;" />

  

  It is not necessary for every output value in the feature map to correspond to every pixel value in the input image. Only the receptive field, where the filter is being used, needs to be connected. Convolutional (and pooling) layers are frequently referred to as "partially connected" layers, which can alternatively be described as local connectivity, because the output array does not have to map exactly to each input value.

  A technique known as parameter sharing, involves keeping the feature detector's weights constant while it traverses over the image. Some parameters, like as weight values, change during training via the backpropagation and gradient descent processes. Prior to starting the neural network's training, three hyperparameters that determine the output's volume size must be established. These include:

  1. The depth of the output is influenced by the number of filters. Three distinct filters, for instance, would result in three different feature maps, giving a depth of three. 

  2. The kernel's stride is how many pixels or how far it travels across the input matrix. Despite the rarity of stride values of two or higher, a longer stride results in a lesser output.

  3. The process of symmetrically adding zeroes to the input matrix is known as [zero-padding](https://medium.com/@draj0718/zero-padding-in-convolutional-neural-networks-bf1410438e99). It is a widely used modification that enables the input size to be changed to meet the needs. It's typically utilized when the filters don't work with the input image. This results in a larger or equally sized output by setting any elements that are not part of the input matrix to zero. There are three different kinds of padding:
  
     - Valid padding: This is sometimes referred to as no padding. In this situation, if dimensions do not line up, the final convolution is dropped.
       
     - Same padding: By using this padding, the output layer is guaranteed to be the same size as the input layer.
       
     - Full padding: This kind of padding enlarges the output by padding the input border with zeros.
  
  
  A CNN performs a [Rectified Linear Unit (ReLU)](https://arxiv.org/pdf/1803.08375.pdf) adjustment on the feature map following each convolution operation, adding nonlinearity to the model.
  
  The first convolution layer may be followed by another. When this occurs, the CNN's structure may become hierarchical because the later layers will be able to view the pixels in the earlier layers' receptive fields. Example of whether an image includes bicycles or not can be taken into consideration. The bicycle can be thought of as a collection of parts. It has a frame, handlebars, wheels, pedals, and other parts. A feature hierarchy is created within the CNN by the bicycle's component pieces, each of which represents a lower-level pattern in the neural network and the bicycle as a whole a higher-level pattern.
  
  
  
  <img src="https://1.cms.s81c.com/sites/default/files/2020-10-20/Feature%20Hierarchy.jpg" alt="Diagram of an feature hierarchy in convolutional neural nets" style="zoom: 33%;" />
  
  
  
  The convolutional layer ultimately transforms the image into numerical data, enabling the neural network to understand and extract pertinent patterns.
  
  
  
- **Pooling layer** - also referred to as downsampling, the pooling layer does dimensionality reduction, which lowers the number of factors in the input. The pooling process sweeps a filter across the entire input, much like the convolutional layer does, however this filter doesn't contain any weights. Instead, the kernel fills the output array with values from the receptive field using an aggregation function. There are two main types of pooling:

  - Max pooling: The filter chooses the input pixel with the highest value to send to the output array as it advances across the input. This method is applied more frequently than average pooling.
  - Average pooling: The average value inside the receptive field is determined as the filter advances over the input, and it is then sent to the output array.

  The pooling layer loses a lot of information, but it also offers the CNN a number of advantages. It lessens complexity, increases effectiveness, and lowers the risk of overfitting.

  

- **Fully-connected (FC) layer** - The full-connected layer is exactly what its name implies. As was already established, with partially linked layers, the input image's pixel values are not connected directly to the output layer. In contrast, every node in the output layer of the fully-connected layer is directly connected to a node in the layer above it.

  This layer conducts the classification operation using the features that were extracted using the various filters and preceding layers. While convolutional and pooling layers frequently utilise ReLu functions to categorise inputs, FC layers typically use a [softmax activation function](https://towardsdatascience.com/softmax-activation-function-how-it-actually-works-d292d335bd78) to do so, resulting in a probability ranging from 0 to 1.

​	With the release of fresh datasets like [MNIST](http://yann.lecun.com/exdb/mnist/) and  [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) and contests like the [ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](https://arxiv.org/pdf/1409.0575.pdf), a variety 	of variant CNN designs have arisen. Other architectures include:

- [AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)
- [VGGNet](https://arxiv.org/pdf/1409.1556.pdf) 
- [GoogLeNet](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43022.pdf) 
- [ResNet](https://arxiv.org/pdf/1512.03385v1.pdf) 
- [ZFNet](https://arxiv.org/pdf/1311.2901v3.pdf)



### Recurrent Neural Networks 

An artificial neural network that employs [sequential data](https://web.engr.oregonstate.edu/~tgd/publications/mlsd-ssspr.pdf) or [time series data](https://arxiv.org/pdf/1809.04356.pdf) is called a [Recurrent Neural Network (RNN)](https://arxiv.org/pdf/1912.05911.pdf). These deep learning algorithms are included into well-known programs like [Siri](https://machinelearning.apple.com/research/hey-siri), [Voice Search](https://ai.googleblog.com/2015/08/the-neural-networks-behind-google-voice.html), and [Google Translate](https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html). They are frequently employed for ordinal or temporal problems, including language translation, natural language processing (NLP), speech recognition, and image captioning. Recurrent neural networks (RNNs) use training data to learn, just like [feed-forward](https://deepai.org/machine-learning-glossary-and-terms/feed-forward-neural-network) and convolutional neural networks (CNNs) do. They stand out due to their "memory," which allows them to affect the current input and output by using data from previous inputs. Recurrent neural networks' outputs are dependent on the previous parts in the sequence, unlike typical deep neural networks, which presume that inputs and outputs are independent of one another. Unidirectional recurrent neural networks are unable to take into account future events in their forecasts, despite the fact that they would be useful in deciding the output of a particular sequence.

The way that RNNs and feed-forward neural networks channel information gives them their names.
The only direction in which information can go in a feed-forward neural network is from the input layer to the output layer via the hidden layers. Without ever touching a node twice, the information travels straight through the network. Having no memory of the input they receive, feed-forward neural networks do poorly at making predictions about the future. A feed-forward network lacks any understanding of chronological order because it solely takes the current input into account. Simply said, it is just capable of recalling the training and nothing else from the past.
Information is cycled around a loop in an RNN. It considers both the current input and the lessons learnt from the inputs it has already received while making decisions.

<img src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/rnn-vs-fnn.png" alt="rnn vs fnn " style="zoom: 50%;" />

​															 	 [_Difference in information flow between a RNN and a feed-forward neural network_](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/rnn-vs-fnn.png)

The fact that recurrent networks share parameters among all of their layers makes them stand out from other types of networks. In contrast to feed-forward networks, which use separate weights for each node, recurrent neural networks use a single weight parameter for all of the network's layers. Nevertheless, to aid with [reinforcement learning](https://arxiv.org/pdf/2005.14419.pdf), these weights are still modified using processes of [backpropagation](https://www.researchgate.net/publication/242202025_NEURAL_NETWORKS_AND_BACK_PROPAGATION_ALGORITHM#read) and [gradient descent](https://arxiv.org/pdf/1903.03614.pdf).

In order to calculate the gradients, recurrent neural networks use the [Backpropagation Through Time (BPTT)](https://axon.cs.byu.edu/~martinez/classes/678/Papers/Werbos_BPTT.pdf)  algorithm, which differs slightly from conventional backpropagation because it is designed for sequence data. BPTT works on the same principles as conventional backpropagation, in which the model learns by computing errors from its output layer to its input layer. The model's parameters can be modified and properly fit thanks to these calculations. In contrast to the conventional method, BPTT adds errors at each time step. This is because feed-forward networks do not share parameters within layers, whereas BPTT does.

<img src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/propagatiom-rnn.png" alt="img" style="zoom: 50%;" />

​																	    [_Illustration of a forward and backward propagation in a neural network_](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/propagatiom-rnn.png)

RNNs frequently experience the two issues of "exploding gradients" and "vanishing gradients" throughout this process. The gradient's size, or the slope of the loss function along the error curve, is what categorizes these problems. The weight parameters are updated until they are no longer significant, or zero, when the gradient is too small. The algorithm stops learning when something happens. When the gradient is too great, exploding gradients happen, which makes the model unstable. In this scenario, the model weights will eventually become too enormous and be represented as NaN. Getting rid of some of the complexity in the RNN model by reducing the number of hidden layers in the neural network is one way to address these problems.

Different varieties of RNNs are employed for various use cases since the lengths of their inputs and outputs might differ. As depicted in the image, there are various kinds of RNNs:

<img src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/Feed-Forward-Neural-Networks.png" alt="Feed Forward Neural Networks" style="zoom:50%;" />

​																					

### Long Short-Term Memory

Recurrent neural networks (RNNs) can extend their memory by using [Long short-term memory networks (LSTMs)](https://arxiv.org/pdf/1909.09586.pdf). As a result, they are well adapted to draw lessons from significant events with significant gaps in time. They were developed as a remedy for the disappearing gradient issue by [Sepp Hochreiter](https://www.jku.at/institut-fuer-machine-learning/ueber-uns/team/sepp-hochreiter/) and [Juergen Schmidhuber](https://www.itu.int/en/ITU-T/AI/Pages/schmidhuber.aspx). Long-term dependencies are a concern that they attempt to address in their study. To put it another way, the RNN model might not be able to predict the present state accurately if the previous state that is influencing the prediction is not recent.

In the neural network's hidden layers, LSTMs have "cells" that have three gates: an input gate, an output gate, and a forget gate. These gates regulate the information flow required to predict the network's output, deciding whether to allow fresh input (input gate), discard the information because it is unimportant (forget gate), or allow it to affect the output at the current timestep (output gate).

<img src="https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/rnn-three-gates.png" alt="img" style="zoom:50%;" />

​																					          [_An illustration of a RNN with its three gates_](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/rnn-three-gates.png)





## Generative Adversarial Networks

In order to model the distribution of training samples and characterize the phenomena in a particular dataset, a method of [unsupervised machine learning](https://www.ibm.com/cloud/learn/unsupervised-learning#) known as [Generative modelling](https://arxiv.org/pdf/2103.04922.pdf) is used. This approach increases the likelihood that predictions of all kinds of probability on a subject may be made from the modelled data.
Generic modelling algorithms process massive amounts of training data in unsupervised machine learning and distil the data down to its digital core. These models often use neural networks and can learn to recognize the data's unique natural characteristics. In order to represent data that is comparable to or identical to actual world data, neural networks use these simplified underlying understandings of real world data.

[Generative Adversarial Networks](https://arxiv.org/pdf/1406.2661.pdf), or GANs, are a method of generative modelling and are a subset of generative models. Deep learning models like convolutional neural networks are utilized by GANs (CNN). In the 2015 publication titled "[Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434)", [Alec Radford](https://github.com/Newmu) established a standardized method known as Deep Convolutional Generative Adversarial Networks, or DCGAN, that produced more reliable models.
It's quite clever how generative models are trained using GANs because the problem is framed as a [supervised machine learning](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7431677/) problem with two sub-models: the generator model, which is trained to generate new examples, and the discriminator model, which tries to categorize examples as either real (from the domain) or fake (a generated one). Two models are combined for adversarial training in a [zero-sum game](https://cs.stanford.edu/people/eroberts/courses/soco/projects/1998-99/game-theory/zero.html) until the discriminator model is tricked around half the time, indicating the generator model is producing credible examples.

<img src="https://machinelearningmastery.com/wp-content/uploads/2019/04/Example-of-the-Generative-Adversarial-Network-Model-Architecture.png" alt="Example of the Generative Adversarial Network Model Architecture" style="zoom: 80%;" />

​                                                                                       [_Generative Adversarial Network Model Architecture_](https://machinelearningmastery.com/wp-content/uploads/2019/04/Example-of-the-Generative-Adversarial-Network-Model-Architecture.png)

Generative modelling offers a route to additional modelling training in difficult domains or areas with less data. In this use case, GANs have had a lot of success in fields like [deep reinforcement learning](https://arxiv.org/pdf/1810.06339.pdf).



### The Generator Model

A fixed-length random vector is sent to the generator model, which then creates a sample within the domain. The vector is chosen at random from a [Gaussian distribution](https://ai.plainenglish.io/gaussian-distribution-in-machine-learning-fa6b2a53ab53) and is used as the process's seed. Following training, this multidimensional vector space's points will match those in the issue domain, resulting in a compressed representation of the data distribution.
Latent spaces, or vector spaces made up of latent variables, are what this particular vector space is known as. Latent variables, often known as hidden variables, are factors that are significant for a domain but cannot be observed directly.
The observed raw data, such as the input data distribution, are compressed or presented in a high-level manner using a latent space. In the case of GANs, the generator model assigns meaning to points in a predetermined latent space, allowing fresh points selected from the latent space to be supplied to the generator model as input and utilized to generate brand-new, distinct output examples. The generator model is retained and used to create fresh samples after training.

<img src="https://machinelearningmastery.com/wp-content/uploads/2019/04/Example-of-the-GAN-Generator-Model.png" alt="Example of the GAN Generator Model" style="zoom: 80%;" />

​																							    [_Illustration of the GAN Generator Model_](https://machinelearningmastery.com/wp-content/uploads/2019/04/Example-of-the-GAN-Generator-Model.png)



### The Discriminator Model

The discriminator model predicts a binary class label of real or fake (generated) based on an input example from the domain (either actual or fabricated). The training dataset contains the actual example. The generator model outputs the created examples. A normal [classification model](https://medium.com/fuzz/machine-learning-classification-models-3040f71e2529) is used as the discriminator. The discriminator model is abandoned after training since the generator is the point of interest. The generator can occasionally be put to new uses since it has mastered the art of successfully extracting features from examples in the issue area. The same or similar input data can be used with some or all of the feature extraction layers in transfer learning applications.

<img src="https://machinelearningmastery.com/wp-content/uploads/2019/04/Example-of-the-GAN-Discriminator-Model.png" alt="Example of the GAN Discriminator Model" style="zoom:80%;" />

​                                                                                           [_Illustration of the GAN Discriminator Model_](https://machinelearningmastery.com/wp-content/uploads/2019/04/Example-of-the-GAN-Discriminator-Model.png)



In this case, zero-sum implies that the discriminator is rewarded or no modification to the model parameters is required when it correctly distinguishes between real and false samples, however the generator is penalized with significant updates to the model parameters. As an alternative, if the generator manages to trick the discriminator, it is rewarded or the model parameters do not need to be changed, but the discriminator is penalized and its model parameters are changed. At some point, the generator will always produce perfect copies of the input domain, rendering the discriminator unable to distinguish between the two and predicting "unsure" (e.g., 50% for real and fake) in every instance.



### GANs and ConvNets

Convolutional Neural Networks, or CNNs, are frequently used by GANs as the generator and discriminator models when working with pictorial data. This is due to the fact that the technique was first described in the context of computer vision, where CNNs and image data were used, as well as the remarkable advancements made in recent years in the use of CNNs more generally to produce cutting-edge outcomes on a variety of computer vision tasks, including [object detection](https://arxiv.org/pdf/1807.05511.pdf) and [face recognition](https://arxiv.org/pdf/1804.06655.pdf).
When modelling image data, a compressed representation of the collection of images or photographs used to train the model is provided by the latent space, the generator's input. Additionally, it implies that the generator creates fresh pictures or photos, producing a result that model creators or users can quickly view and evaluate.
The focus of computer vision applications with CNNs and on the enormous improvements in capabilities of GANs as compared to other generative models has been driven by the ability to visually evaluate the quality of the generated output.



### Conditional GANs

Using the GAN to conditionally generate an output is a significant development of the technology. When the input, a random vector drawn from the latent space, is given with (conditioned by) some extra input, the generative model can be trained to produce fresh examples from the input domain. In order to create images of handwritten numbers, the additional input may be a digit or a class value, such as male or female in the case of creating photos of individuals.
Additionally, the discriminator is conditioned, which entails giving it both the additional input and an input image that can either be true or fake. When a conditional input of the classification label type is used, the discriminator will assume that the input belongs to that class, which will educate the generator to produce instances from that class in order to deceive the discriminator. A conditional GAN can produce examples from a domain of a specific kind in this manner.
A step further is to condition the GAN models on a domain example, like an image. This enables the use of GANs for tasks like [text-to-image translation](https://github.com/bunny98/Text-to-Image-Using-GAN) or [image-to-image translation](https://arxiv.org/pdf/1611.07004.pdf). As a result, it is possible to use GANs for some of its more amazing applications, like style transfer, photo colorization, changing images from summer to winter or day to night, and so forth.
The discriminator is given samples of real and created nighttime photos as well as (conditioned on) real daytime photos as input in the case of conditional GANs for image-to-image translation, such as converting day to night. In addition to actual daytime images, the generator is given a random vector from the latent space as input.

<img src="https://phillipi.github.io/pix2pix/images/teaser_v3.png" alt="img" style="zoom:33%;" />



### GANs and Images

One of the many significant developments in the application of deep learning techniques in fields like computer vision is a process known as [data augmentation](https://arxiv.org/pdf/1711.04340.pdf). Improved model performance is the outcome of data augmentation, which boosts model expertise and has a regularizing effect that lowers generalization error. It functions by generating brand-new, synthetic, but realistic instances from the input issue area on which the model is trained.
For image data, the procedures are crude and involve cropping, flipping, zooming, and other straightforward transformations of the training dataset's existing images.
Generative modelling offers a way to increase modelling training in complex domains or areas with little data. In this use case, GANs have achieved great success in fields like deep reinforcement learning.

GANs are intriguing, significant, and demand additional investigation for a variety of reasons in the field of research. In his 2016 conference keynote and related technical report titled "[NIPS 2016 Tutorial: Generative Adversarial Networks](https://arxiv.org/abs/1701.00160)", [Ian Goodfellow](https://www.iangoodfellow.com/) lists a few of these. He emphasizes the success of GANs in modelling high-dimensional data, handling missing data, and the potential of GANs to produce multi-modal outputs or numerous plausible explanations.

Conditional GANs for tasks requiring the creation of fresh examples are where GANs are most effectively used. Goodfellow provides three key instances:

- Image Super-Resolution: Creating high-resolution copies of the input photos.
- Creating Art: The ability to create original, artistic drawings, paintings, and more.
- Image-to-Image Translation: The ability to convert images between different climes, including day and night, summer and winter, and more.

One of the major reasons that GANs are widely studied, developed, and used is because of their success. GANs have been able to generate photos so realistic that humans are unable to tell that they are of objects, scenes, and [people that do not exist in real life](https://this-person-does-not-exist.com/en).

The success of GANs is one of the main causes for the widespread study, creation, and application of these algorithms. Humans are unable to distinguish between [people created by GANs](https://this-person-does-not-exist.com/en) and real people because they are so lifelike.

<img src="https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2019/02/dnefe.jpg" alt="img" style="zoom:50%;" />

​                                                                                      [_Pictures generated from This Person Does Not Exist_](https://cdn0.tnwcdn.com/wp-content/blogs.dir/1/files/2019/02/dnefe.jpg)





## Diffusion Probabilistic Models

In the last few years, generative models called [Diffusion Models](https://arxiv.org/pdf/2006.11239.pdf) have been increasingly popular. The world has seen what diffusion models are capable of, such as [outperforming GANs on image synthesis](https://arxiv.org/pdf/2105.05233.pdf), thanks to a select few landmark publications published in the 2020s. Practitioners most recently saw the usage of diffusion models in [DALL-E 2](https://medium.com/augmented-startups/how-does-dall-e-2-work-e6d492a2667f), OpenAI's image creation model that was released in the beginning of 2021. Due to their practical performance, the [VAE](https://arxiv.org/pdf/1906.02691.pdf), GAN, and [Flow](https://lilianweng.github.io/posts/2018-10-13-flow-models/) family of models have dominated the area for the past few years. Despite their commercial success, these generative models have theoretical and design flaws (intractable likelihood computation, constrained architecture, unstable training dynamics, etc.) that have led to the development of a new class of generative models known as "Diffusion Probabilistic Models" or DPMs. They were initially proposed by [Sohl-Dickstein et al. 2015](http://proceedings.mlr.press/v37/sohl-dickstein15.html), and are based on the [thermodynamics of the diffusion process](https://arxiv.org/pdf/1412.5925.pdf). They learn a noise-to-data mapping in discrete steps, very much like Flow models.

<img src="https://developer-blogs.nvidia.com/wp-content/uploads/2022/04/Fixed_Forward_Diffusion_Process.png" alt="An image of a cat is perturbed from left to right by adding Gaussian noise. Arrows indicate fixed forward diffusion and generative reverse denoising processes."  />

​                                                                             [*Diffusion model processes moving to and from data and noise*](https://developer-blogs.nvidia.com/wp-content/uploads/2022/04/Fixed_Forward_Diffusion_Process.png)

[Non-equilibrium thermodynamics](https://royalsocietypublishing.org/doi/10.1098/rsta.2020.0066) serves as the basis for diffusion models. They learn to reverse the diffusion process to create desired data samples from the noise after defining a [Markov chain](https://proceedings.neurips.cc/paper/2018/file/d34ab169b70c9dcd35e62896010cd9ff-Paper.pdf) of diffusion steps to gradually introduce random noise to data. Diffusion models, in contrast to VAE or flow models, are trained using a predefined technique, and the latent variable is highly dimensional (same as the original data).

In generative modelling, [tractability](https://arxiv.org/pdf/1905.07026.pdf) and flexibility are two competing goals. Tractable models are amenable to analytical evaluation and low-cost data fitting (e.g., via a Gaussian or Laplace), but they struggle to capture the complexity of rich datasets. Flexible models can accommodate any type of data format, but they are typically expensive to evaluate, train, or sample from. Diffusion models are adaptable and analytically traceable. However, they rely on a lengthy Markov chain of diffusion stages to generate samples, which makes them quite time and computation intensive. Although new approaches have been put forth to speed up the procedure significantly, sampling is still slower than GAN.





## Latent Spaces

[Latent spaces](https://arxiv.org/pdf/2112.04895.pdf) are described as abstract, multidimensional spaces that store significant internal representations of events that have been experienced externally. In the latent space, samples that are comparable in the real world are situated close to one another.
Since each witnessed event is stored in a condensed picture in the brain, human beings are able to comprehend a wide range of subjects. For instance, they don't remember every nuance of a cat's appearance such that we can spot one in the street. They maintain an internal model of a cat's typical appearance. Latent space seeks to give a computer a grasp of a wide range of topics and the events associated with those topics comparable to how humans do so, through a quantitative spatial representation or modelling.
Because learning the characteristics of data and streamlining data representations with the intention of identifying patterns are at the heart of deep learning, the concept of latent spaces is crucial. The reason to learn a latent space over seen data (a series of events) is that significant differences in observed space or events may be caused by minute alterations in latent space (for the same topic). Since seen data itself is a very large area to learn from, learning a latent space would help the model make sense of observed data more effectively.



### Data Compression 

The practice of encoding information using fewer bits than the original representation is known as [data compression](https://bair.berkeley.edu/blog/2019/09/19/bit-swap/). There are two types of compression: [lossy and lossless](https://www.researchgate.net/publication/2739013_Lossless_and_Lossy_Data_Compression#read). By locating and removing statistical redundancy, lossless compression lowers the number of bits. Lossless compression does not result in information loss. Bits are reduced by lossy compression by excluding irrelevant or less significant data. A device that conducts data compression is typically referred to as an [encoder](https://www.encoder.com/article-what-is-an-encoder#), whereas a device that performs data decompression is referred to as a [decoder](https://www.sciencedirect.com/topics/mathematics/decoder).

<img src="https://miro.medium.com/max/578/0*zG3k_ciZomNRaO-K.jpg" alt="img" style="zoom: 80%;" />

​																				                   	[*Illustration of compressed data*](https://miro.medium.com/max/578/0*zG3k_ciZomNRaO-K.jpg)

For example, if a [fully convolutional neural network (FCN)](https://arxiv.org/pdf/1411.4038.pdf) is required to be trained to classify an image (that is, produce a digit number given a digit image), edges, angles, and other features are just learned at each layer as the model "learns" by associating a set of features with a certain output.
However, the dimensionality of the image is initially decreased before it is finally enhanced each time the model learns from a data point. This is a type of lossy compression as the dimensionality is decreased. The model must learn to save all pertinent information and ignore noise because it must later recover the compressed data. The benefit of compression is that it enables us to eliminate any unnecessary data and concentrate solely on the most crucial elements. The Latent Space Representation of data is what is meant by this "compressed state."

If the original dataset contains pictures with the dimensions [5 x 5 x 1](https://miro.medium.com/max/216/0*rZn-ksyRTUpDUxh7), then the compressed data point is a 3-dimensional [vector](https://library.leeds.ac.uk/info/14012/mathematics/60/vectors) since the latent space dimensions are set to [3 x 1](https://miro.medium.com/max/37/0*GRY4C_Ov5RYMn6Vi).

![img](https://miro.medium.com/max/216/0*rZn-ksyRTUpDUxh7)

![img](https://miro.medium.com/max/37/0*GRY4C_Ov5RYMn6Vi)

Only 3 numbers are used to uniquely identify each compressed data piece. Therefore, this data can be graphed on a 3D plane. This is what "space" is being referred to.

<img src="https://miro.medium.com/max/700/1*djUvYgePXdjMbdbGsDq2eg.png" alt="img" style="zoom: 67%;" />

​                                                                                             [*Point (0.4, 0.3, 0.8) graphed in 3D space*](https://miro.medium.com/max/700/1*djUvYgePXdjMbdbGsDq2eg.png)



Since humans are 3-dimensional beings, n-dimensional space (where n > 3) is beyond human comprehension. In light of this, there are tools like [t-SNE](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) that can convert higher dimensional latent space representations into ones that can be visualized by the human brain (2D or 3D).



### Latent Spaces in Autoencoders and Generative Models

The [autoencoder](https://arxiv.org/pdf/2003.05991.pdf), a neural network that serves as an identity function, is a popular deep learning model that modifies the 'closeness' of input in the latent space. To put it another way, an autoencoder picks up on how to output whatever is given as an input. The priority in the working of an autoencoder is not given to what the model outputs, which is basically the input, but rather what the model learns in the process.
When a model is made into an identity function, it is compelled to store all of the data's pertinent features in a compressed representation in order to have enough information in that compressed form for the model to "accurately" reconstruct the data. The latent space is represented by this compressed representation, as shown by the red blocks in the image illustrated.

<img src="https://miro.medium.com/max/700/0*dknVUIxkjQ3ZV8y0.png" alt="img" style="zoom: 67%;" />

​																							     [*General architecture of an Autoencoder*](https://miro.medium.com/max/700/0*dknVUIxkjQ3ZV8y0.png)



### Latent Spaces and GANs

[Interpolation](https://arxiv.org/pdf/2110.06421.pdf) in latent spaces can be defined as compressing and then sampling points in the latent space between two distinct clusters in the latent space to generate new results, although they might not be independent of the original data. Below is an example of [linear interpolation](https://arxiv.org/pdf/2105.03663.pdf) of a latent space between 2 different types of chairs.

![img](https://miro.medium.com/max/480/0*cYaaF2pFLECohCaI.gif)

​																					                 	[*Interpolation in latent space*](https://miro.medium.com/max/480/0*cYaaF2pFLECohCaI.gif)



To generate new images via a GAN, points can be sampled from a latent space. For example, by using a model decoder to rebuild the latent space representation into a 2D image with the same dimensions as the original input, various facial structures can be generated by interpolating on the latent space.

<img src="https://miro.medium.com/max/700/0*6Nrb168aK7xoaV8b.png" alt="img" style="zoom:50%;" />

​                                                                                       [*Image generation through latent space interpolation*](https://miro.medium.com/max/700/0*6Nrb168aK7xoaV8b.png)





## Prompt Engineering

In natural language processing (NLP), a concept known as [prompt engineering](https://arxiv.org/pdf/2109.01134v5.pdf) entails identifying inputs that produce outputs that are preferable or beneficial. In prompt engineering, the task description is included explicitly in the input, such as a question, as opposed to being provided implicitly. Typically, prompt engineering involves transforming one or more tasks into a prompt-based dataset and "prompt-based learning", also known as "prompt learning" to train a language model. Prompt engineering, also known as "prefix-tuning" or "prompt tuning," is a method wherein a big, "frozen" pretrained language model is used and just the prompt's representation is learnt. 

Text-based prompts may be created using prompt engineering to get the desired picture categorization results. The model may be instructed to display "an picture of potatoes," for instance. Prompt engineering is fundamentally dependent on the form of such prompts, or the statement describing how the model detects pictures. Iterative writing is frequently necessary to come up with the finest prompt. A "snapshot of potatoes", "a gathering of potatoes", or the prompt "an image including potatoes" are all quite different things. 
The quality of the inputs influences the quality of the outputs in most processes. It's more likely that the model will provide a constructive and relevant response when the prompts are well-designed. Understanding what the model "knows" about the world and then applying that knowledge appropriately are key to creating effective prompts.

When creating model prompts, a few key ideas should be kept in mind:

- **The model is guided by a prompt to provide usable output** - If a big language model with enough training data is instructed to write a summary of an article, for instance, it should be done like as illustrated below:

  <img src="https://docs.cohere.ai/img/prompt-engineering/summarization-prompt-language-model.png" alt="img" style="zoom: 67%;" />

​                                                               [*This prompt has two components: the text that has to be summarized, and the task description*](https://docs.cohere.ai/img/prompt-engineering/summarization-prompt-language-model.png)



- **To achieve the finest generations, experiment with different formulations of a prompt** - When using a generator, it might be helpful to experiment with a variety of alternative prompts for the situation at hand. Even while multiple ways of phrasing the same prompt may seem similar to humans, this might result in generations that are very distinct from one another. This may occur, for example, as a result of the model's knowledge that the various formulations are really employed in a variety of circumstances and for a variety of objectives.
  If "In summary" in the preceding example doesn't provide the desired results, "To summarize altogether" or "The key takeaway from this text is that" can be used.

  

- **Describe the assignment and the general surroundings** - Additional task description elements are frequently helpful, and they usually occur after the input text that has to be processed.

  <img src="https://docs.cohere.ai/img/prompt-engineering/prompt-task-description.png" alt="img" style="zoom:67%;" />

Give the model sufficient context. For instance, before the article, the summarization task can be described in greater depth.

<img src="https://docs.cohere.ai/img/prompt-engineering/prompt-summary-example.png" alt="img" style="zoom:67%;" />



Another use case of such language models can be to help a customer satisfaction department by using them to create realistic customer answers automatically -

A customer raises the following query to a company:

``` html
Hi, I'd like a refund for the coffee maker I ordered. Would that be possible?

```

In order to provide valuable generation for the agent working with the client, start by explaining to the model the overall situation and what the rest of the prompt will be about:

``` html
This is a conversation between a customer and a polite, helpful customer service agent.
Question of the customer: Hi, I'd like a refund for the coffee maker I ordered. Would that be possible?
```

The model has been informed on what to anticipate, and it is evident from the query that it is a customer-related one. Next, let's display to the model the first portion of the answer that is to be provided to the client:

``` html
Response by the customer service agent: Hello, thank you for reaching out to us. Yes,
```

Notice how it is made apparent that the next statement is an answer to the query, that it comes from a customer service representative, and that favorable response is to be given. Combining all of this yields the following question:

``` html
This is a conversation between a customer and a polite, helpful customer service agent.
Question of the customer: Hi, I'd like a refund for the coffee maker I ordered. Would that be possible?
Response by the customer service agent: Hello, thank you for reaching out to us. Yes,
```

<img src="https://docs.cohere.ai/img/prompt-engineering/generation-prompt-example.png" alt="an example of a prompt with task description, input indicator, current input, and output indicator" style="zoom:67%;" />

[*When using several examples in the prompt, some prompt elements (such input and output indicators) are helpful in communicating a desired task to the model*](https://docs.cohere.ai/img/prompt-engineering/generation-prompt-example.png)

​		In this instance, a few customer service interactions are sufficient to obtain credible completions from the baseline model. This might be further enhanced by 		honing it using examples of how one wants the model to respond to different inquiries and requests.



- **The preferred outcome should be shown to the model** - One of the primary strategies for producing effective generations is to supplement a prompt with examples. Examples show the model what kind of outcome is being aimed for.

  <img src="https://docs.cohere.ai/img/prompt-engineering/prompt-examples.png" alt="Prompt with task description and two examples" style="zoom:67%;" />

List a few examples to get different generations. This technique is known as few-shot learning. For example, if a movie review was to be categorized as 	favorable, unfavorable, or neutral, and the model is given the prompt as:

``` html	
Review: "I really enjoyed this movie!"
This sentiment of this review is
```

The model might generate something like:

``` html
This sentiment of this review is apt, considering the movie's plot,
```

In such a case, the model predicts certain generations that are not the kind of generations as expected.



<img src="https://docs.cohere.ai/img/prompt-engineering/prompt-example-input-output.png" alt="Example inputs and outputs in the prompt" style="zoom:67%;" />

​                                                          [*Both an example input and the desired output should be included in the examples in the prompt*](https://docs.cohere.ai/img/prompt-engineering/prompt-example-input-output.png)



``` html
This is a movie review sentiment classifier.
Review: "I loved this movie!"
This review is positive.
Review: "I don't know, it was ok I guess.."
This review is neutral.
Review: "What a waste of time, would not recommend this movie."
This review is negative.
Review: "I really enjoyed this movie!"
This review is positive.
```



This prompt may be seen in a more straightforward form as shown below:

<img src="https://docs.cohere.ai/img/prompt-engineering/prompt-engineering-detailed-examples.png" alt="Example of a prompt with two examples" style="zoom:67%;" />





## Style Transfer

### Transfer Learning 

A model created for one task is used as the basis for another using the machine learning technique known as [transfer learning](https://arxiv.org/pdf/1911.02685.pdf). Pre-trained models are frequently utilized as the foundation for deep learning tasks in computer vision and natural language processing because they save both time and money compared to developing neural network models from scratch and because they perform far better on related tasks. This optimization enables quick development or better performance when modelling the second task. 

<img src="https://assets-global.website-files.com/5d7b77b063a9066d83e1209c/616b35e31e352613b291f4c7_P46neY1rhMd5NbTxhCbaWWAnwJwhgCZjSLp2C2-6Pzf8sBEfAxtlhnAOV_Jq_gX-zOaztDWLrtFal42V-EDr86Gcd8QYrWh4uMxZ-_-X_Pd5tOge9EkBmFr7UxrEWLMwCNZi14WK%3Ds0.jpeg" alt="Transfer learning" style="zoom: 67%;" />



Transfer learning is not just a topic for deep learning research; it also deals with issues like [concept drift](https://arxiv.org/pdf/2004.05785.pdf) and [multi-task learning](https://arxiv.org/pdf/2009.09796.pdf). Transfer learning, however, is often used in deep learning due to the substantial resources needed to train deep learning models or the big and difficult datasets that deep learning models are trained on. Only general model characteristics that were learnt from the initial task can be used for transfer learning in deep learning. Transfer learning is frequently used in computer vision and natural language processing applications like [sentiment analysis](https://arxiv.org/ftp/arxiv/papers/2006/2006.03541.pdf) because to the enormous amount of CPU power that is needed.

Layered architectures used in deep learning systems enable the learning of various features at various layers. Higher-level characteristics are compiled at the network's outermost layers, and as we move deeper into the network, they get more precise.

To obtain the final output, these layers are eventually linked to the final layer, which is often a completely connected layer in the case of supervised learning. This makes it possible to use well-known pre-trained networks, such as the [Oxford VGG Model](https://www.robots.ox.ac.uk/~vgg/research/very_deep/), [Google Inception Model](https://cloud.google.com/tpu/docs/inception-v3-advanced), and [Microsoft ResNet Model](https://www.microsoft.com/en-us/research/blog/microsoft-vision-model-resnet-50-combines-web-scale-data-and-multi-task-learning-to-achieve-state-of-the-art/) for additional tasks without relying on their last layer as a fixed feature extractor.

<img src="https://miro.medium.com/max/1400/1*qfQ3hmHLwApXZBN-A85r8g.png" alt="A Comprehensive Hands-on Guide to Transfer Learning with Real-World  Applications in Deep Learning | by Dipanjan (DJ) Sarkar | Towards Data  Science" style="zoom:67%;" />

​                                                               [*Transfer Learning with Pre-trained Deep Learning Models as Feature Extractors*](https://miro.medium.com/max/1400/1*qfQ3hmHLwApXZBN-A85r8g.png)



The trick here is to use the weighted layers of the trained model to extract features, but not to change the weights of the model while training with fresh data for the new task. Pre-trained models can be used to provide a broad representation of the visual world since they were trained on sufficiently big and diverse datasets.

In computer vision, neural networks generally target the first layer's detection of edges, the middle layer's detection of shapes, and the latter layers' detection of task-specific properties. Transfer learning only uses the early and center layers; the later layers are only retrained. It uses the labelled data from the task that served as its training ground.

<img src="https://learnopencv.com/wp-content/uploads/2019/05/transfer-learning-1024x574.jpg" alt="img" style="zoom: 50%;" />



Reduced training time, enhanced neural network performance (in most cases), and the lack of a significant quantity of data are three of transfer learning's most significant advantages. It is useful in situations when it is not always possible to get the large amounts of data required to train a neural network from scratch.

<img src="https://editor.analyticsvidhya.com/uploads/35504classifiers-transfer-learning.jpeg" alt="   Why Should You Use Transfer Learning?" style="zoom:67%;" />



Transfer learning can provide an effective machine learning model with very minimal training data since the model has previously been trained. This is particularly helpful in NLP, where handling vast labelled datasets necessitates a high level of expertise. Additionally, training time is shortened because it might take days or even weeks to complete a complicated task after creating a deep neural network from scratch.

There are 2 common ways to implement transfer learning, they are

1. **A Develop Model Approach** involving the following steps:

   1. Choose the Source Task: Choose an issue involving predictive modelling that is connected, has a large amount of data, and where the input, output, and/or ideas discovered when mapping input to output data are all related in some way.

   2. Build the source model: The next step is to create a proficient model for this initial assignment. To confirm that any feature learning has taken place, the model must be superior to a naïve model.

   3. Reuse the model: A model on the second task of interest may be built from the model fit on the source task as a starting point. Depending on the modelling approach employed, this can include utilizing the entire model or only a portion of it.

   4. Tune the model: The model might need to be modified or improved based on the input-output pair data provided for the relevant task.

      

2. **A Pre-Trained Model Approach** which is frequently used in deep learning, involving the following steps:

   1. Choose the Source Model: One of the accessible models is a source model that has already been trained. Many research organizations publish models on sizable and difficult datasets, which may be part of the selection of potential models.

   2. Reuse the model: A model on the second task of interest may then be built using the pre-trained model as a base. Depending on the modelling approach employed, this can include utilizing the entire model or only a portion of it.

   3. Tune the model: On the basis of the input-output pair data available for the relevant task, the model may need to be modified or improved.

      

Transfer learning is a time-saving, expedient method of improving performance. Usually, it is not immediately apparent whether using transfer learning in the domain will be advantageous until after the model has been created and assessed. Hence, there are three advantages  to seek from the use of transfer learning:

1. Higher start: The initial skill on the source model (prior to model refinement) is higher than it otherwise would be.
2. Higher slope: The rate of skill growth during source model training is steeper.
3. Higher asymptote: The trained model's converged skill is superior than what it would be without training.

<img src="https://machinelearningmastery.com/wp-content/uploads/2017/09/Three-ways-in-which-transfer-might-improve-learning.png" alt="img" style="zoom: 50%;" />



Transfer learning may help to create expert models for some issues where there is not a lot of data available. If there is task that is similar to one's own with a lot of data and the resources to build a model for that work and reuse it on the problem, or if there is a pre-trained model that can be used as a starting point for a new model, this approach is very much useful. The selection of the source model or data is an open problem and may call for experience-based intuition or domain knowledge.



### Style Transfer

A computer vision method called [style transfer](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf) enables us to recompose one image's content in the manner of another. It makes hypothetical concepts, such as what a photograph could be like if it were painted by a well-known artist, a reality. 
Style transfer is the process of combining two images - a content picture and a style reference image - so that the final output image keeps the essential components of the content image while also seeming to have been "painted" in the manner of the style reference image.

<img src="https://production-media.paperswithcode.com/thumbnails/task/task-0000000002-6ec5afb9.jpg" alt="img" style="zoom: 80%;" />

​                                                                                      [*The same picture (top left) in multiple different styles*](https://production-media.paperswithcode.com/thumbnails/task/task-0000000002-6ec5afb9.jpg)



In the larger area of [non-photorealistic rendering](https://www.researchgate.net/publication/236973460_Non-Photorealistic_Rendering#read), style transfer is an example of picture stylization, an image processing and editing approach that has been researched for many years. 
An original image and an artistic rendition of that original image are needed for style transfer in a conventional supervised learning technique. A machine learning model can then use the transformation on fresh original photos after learning it.

Due of the rarity of certain picture combinations, this strategy is mostly unfeasible. [Neural style transfer (NST)](https://arxiv.org/pdf/1508.06576.pdf), a novel method, has altered what is achievable recently. Deep neural networks are used by NST to drive these transitions. In order to measure the effectiveness of the style transfer without the usage of explicit picture pairs, neural networks are utilized to extract statistical aspects of images that are connected to content and style. Using this enhanced method, the neural network can apply the learned style representation to original content pictures with just one style reference image.
NST's early version, though, were not without flaws. In order to execute style transfer on a single image, the challenge was approached as an optimization problem that required hundreds or thousands of repetitions. [Fast neural style transfer](https://cs.stanford.edu/people/jcjohns/eccv16/) is a method that researchers devised to address this inefficiency. While using deep neural networks, fast style transfer trains a separate model that can alter any picture in a single feed-forward pass. Instead of using hundreds of iterations via the network, trained models can stylize any image. Modern-day style transfer models even have the capability of learning to imprint several styles using a single model, allowing for the creative editing of a single input content image in an almost limitless number of ways.



### Working of Style Transfer

Convolutional neural network (CNN) variants serve as the foundation for deep learning architectures that are appropriate for style transfer. Making important contrasts between various techniques can be accomplished by following the development of style transfer research. Examining the ongoing advancement of these methodologies and optimizations, from single- and multiple-style model approaches to arbitrary-style model approaches, can help in comprehending the potential of style transfer.

- **Basic Framework** -  A pre-trained feature extractor and a transfer network are required for training a style transfer model. To avoid using paired training data, a feature extractor that has already been trained is employed. Its utility arises from a certain propensity of [individual deep convolutional neural network layers trained for image classification to specialize in comprehending particular aspects of a picture](https://distill.pub/2017/feature-visualization/).

  <img src="https://www.fritz.ai/images/neural_network_feature_visualization.jpg" alt="Neural Network Feature Visualization" style="zoom:80%;" />

  

  

  Some layers learn to focus on texture (the small brush strokes of a painter or the fractal patterns of nature), while others learn to extract the image's information (such as a dog's shape or a car's location). This is taken advantage of by style transfer, which processes two photos through a [pre-trained neural network](https://towardsdatascience.com/4-pre-trained-cnn-models-to-use-for-computer-vision-with-transfer-learning-885cb1b2dfc), examines the output at various levels, and assesses how similar the two images are. Images that give identical outputs at different layers of the pre-trained model indicate comparable content while matching outputs at the same layer indicate similar style.

  Despite not directly assisting in producing the styled picture, the pre-trained model helps to compare the content and style of two photos. A second neural network, referred to as the transfer network, is responsible for it. The transfer network is a network that translates images, taking one picture as input and producing another image as output. In transfer networks, an encoder-decoder design is very common.

  <img src="https://www.fritz.ai/images/fast_style_transfer_arch.jpg" alt="Style Transfer Model Architecture" style="zoom: 80%;" />

  

  The pre-trained feature extractor is first applied to one or more style pictures, and the outputs at different style layers are stored for further comparison. The machine is then given content pictures. The pre-trained feature extractor runs through each content picture, saving results at different content layers. The transfer network then generates a stylized version of the content picture. The feature extractor is likewise applied to the styled picture, and outputs at the content and style layers are stored.

  The unique loss function that determines the stylized image's quality includes keywords for both content and style. While the extracted style aspects are contrasted with those from the reference style picture, the content features from the stylized image are compared to the original content image (s). Only the transfer network is updated following each step. The pre-trained feature extractor's weights remain constant throughout. We may train models to generate output pictures with lighter or greater stylization by weighing the various elements of the loss function.

  

- **Overview of some of the Model Architectures** - To showcase a few robust and reliable ones:

  - Single style per model: The first independent neural network to be trained to stylize pictures in a single feed-forward pass was introduced in a [2016 paper by Johnson et al](https://cs.stanford.edu/people/jcjohns/eccv16/). The feature extractors are large [VGG16](https://arxiv.org/pdf/1409.1556v6.pdf) models pre-trained on ImageNet, whereas the transmission network is a relatively tiny encoder-decoder network. A separate transfer network is trained for each desired style in this method.
  - Multiple styles per model: Researchers at Google expanded the quick style transfer method in 2017, one year after it was first released, to enable a [single transfer network to create pictures in many styles and even blend numerous styles together](https://arxiv.org/pdf/1610.07629.pdf). The inclusion of "conditional instance normalization" layers inside the network, which allowed the stylized picture generated to be conditioned on a different model input, was their significant contribution. These networks accept as input a content picture and a second vector that instructs them how much of each style to apply to the input image. So a model may, for instance, be trained on works by van Gogh, Picasso, and Matisse. The user can enter [1, 0, 0] for van Gogh, [0, 1, 0] for Picasso, or [0.33, 0.33, 0.33] for a combination of all three when it comes time to stylize an image. This is a wonderful method since it eliminates the need to train and store several models for various styles and gives users the ability to combine various styles.
  - Arbitrary styles per model: Both single-style transfer models and multi-style transfer models have the ability to only create pictures in the styles they have previously seen during training. Without retraining the whole network, a model created using van Gogh's artwork cannot create artwork similar to Picasso's. That is altered by Huang and colleagues' arbitrary style transfer. Arbitrary style transfer models accomplish style transfer in a single, feed-forward pass using a content picture and a style image as input. In essence, the model learns how to instantly extract and apply any style to a picture.
  - Extensions and optimizations for style transfer: A few enhancements and optimizations of style transfer are important to note:
    - Stable style transfer comes first. A substantial degree of flickering would be present in the output of a style transfer model that had been trained before being applied to video frame by frame. Distracting temporal inconsistencies can be introduced into a video by even little frame-to-frame variations and noise. A soccer ball, for instance, could change color while in flight. Stable style transfer models address this issue by [introducing a new loss term linked to "temporal coherence"](https://arxiv.org/pdf/1807.01197.pdf), which compares the stylization of two consecutive frames in order to train the model to apply the same stylization to an item as it moves across the frame.
    - [Color preservation](https://arxiv.org/pdf/1606.05897.pdf) is another aspect of style transfer. In certain instances, one may want to reproduce an artist's brushstrokes on a picture while keeping the original palette's colors. The input picture representation from RGB to another color space and applying style transfer exclusively to the luminance channel or using a color transfer algorithm to the final styled image are two methods that may be used to accomplish this.
    - Lastly, though the majority of the style transfer use cases stated so far have added an aesthetic style to a picture image, it is also feasible to transmit other heuristics, like weather or time of day, between two photorealistic photographs. In order to eliminate artefacts brought about by deep convolution and [upsampling layers](https://towardsdatascience.com/transposed-convolution-demystified-84ca81b4baba), [photorealistic style transfer](https://arxiv.org/pdf/1802.06474.pdf) often necessitates adjustments to the encoder-decoder transfer network.



### Importance of Style Transfer

Not everyone is naturally creative. Some people are better at skills requiring touch or words. However, because to recent developments in technologies like style transfer, practically anybody may now experience the delight that comes from making and appreciating a work of art. This is where style transfer has the ability to transform. Artists are able to readily share their aesthetic sensibility with others, allowing for the coexistence of fresh and creative interpretations of many artistic movements with classic works of art. In addition to encouraging people all over the world to explore their own creativity, style transfer is crucial to the world of commercial art. An example is [Christie's recently showcased painting created by AI that fetched more than $430,000 at one of their auctions](https://www.christies.com/features/A-collaboration-between-two-artists-one-human-one-a-machine-9332-1.aspx).

Style transfer is now possible for recorded and live video because to advancements in AI-accelerated technology in the cloud and on the edge. The possibilities for design, content creation, and the advancement of creative tools are unlimited thanks to this new capability.





# A Platform for Generative Art





